# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  
  config.vm.box = "ubuntu/bionic64"
  config.vm.define "datalake" do |datalake|
    datalake.vm.hostname = "datalake"
    datalake.vm.network "private_network", ip: "192.168.33.100"    
    datalake.vm.provider "virtualbox" do |vb|  
      vb.memory = "4096"
    end
    datalake.vm.provision "shell", inline: <<-SHELL
      echo "export HADOOP_HOME=/opt/hadoop" >> /etc/profile
      echo "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64/" >> /etc/profile
      echo "export HDFS_NAMENODE_USER=root" >> /etc/profile
      echo "export HDFS_DATANODE_USER=root" >> /etc/profile
      echo "export HDFS_SECONDARYNAMENODE_USER=root" >> /etc/profile
      echo "export YARN_RESOURCEMANAGER_USER=root" >> /etc/profile
      echo "export YARN_NODEMANAGER_USER=root" >> /etc/profile
      apt clean
      apt update
      apt install python-pip -y
      apt install -y openjdk-8-jre
      apt install openjdk-8-jdk-headless -y
      apt install -y git ant gcc g++ libffi-dev libkrb5-dev libmysqlclient-dev \
                         libsasl2-dev libsasl2-modules-gssapi-mit libsqlite3-dev libssl-dev \
                         libxml2-dev libxslt-dev make maven libldap2-dev python-dev python-setuptools libgmp3-dev      
      cd /opt
      wget http://ftp.unicamp.br/pub/apache/hadoop/common/stable/hadoop-3.2.1.tar.gz
      tar -xf hadoop-3.2.1.tar.gz
      mv -v hadoop-3.2.1 /opt/hadoop
      echo "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64/" >> /opt/hadoop/etc/hadoop/hadoop-env.sh
      cp /vagrant/core-site.xml /opt/hadoop/etc/hadoop/ -v
      cp /vagrant/hdfs-site.xml /opt/hadoop/etc/hadoop/ -v
      source /etc/profile
      yes | /opt/hadoop/bin/hdfs namenode -format
      yes | ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa
      cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
      /opt/hadoop/sbin/start-dfs.sh
    SHELL
  end

  config.vm.define "airflow" do |airflow|
    airflow.vm.hostname = "airflow"
    airflow.vm.network "private_network", ip: "192.168.33.111"
    airflow.vm.provider "virtualbox" do |vb|  
      vb.memory = "2048"
    end
    airflow.vm.provision "shell", inline: <<-SHELL  
      apt clean
      apt update    
      apt install python3 python3-pip -y
      python3 -m pip install pyspark
      python3 -m pip install apache-airflow[postgres,google]==1.10.10 \
      --constraint https://raw.githubusercontent.com/apache/airflow/1.10.10/requirements/requirements-python3.7.txt
      echo "PATH=$PATH:~/.local/bin" >> /etc/bash.bashrc
      apt install openjdk-8-jre-headless -y
      python3 -m pip install hdfs
    SHELL
  end
  
end